{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking1\tALS都有哪些应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要应用于两大场景，第一种是评分预测，主要用于网站的评分，比如用户给自己看过的电影评多少分，或者用户给自己看过的书籍评价多少分，对已有的评分来估计用户对其他物品的评分预测；第二种是Top-N推荐，常用于购物网站，拿不到显式评分，通过用户的隐式反馈为用户提供一个可能感兴趣的商品列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking2\tALS进行矩阵分解的时候，为什么可以并行化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乘法的主要思想是固定一个矩阵求另外一个矩阵，然后再固定另外一个矩阵求解这个矩阵，不断迭代，从而找到数据最匹配的参数。在矩阵求解的过程中，比如固定y求解x,目标评分矩阵为A，x的每一行可以独立求解，通过对x的第i行和y的计算可以得到A的第i行，对于每一步来说，x或者y的行或者列都是可以独立并行求解的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking3\t梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD）\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新；随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新，使得训练速度加快；小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法，其思想是：每次迭代使用一部分样本来对参数进行更新。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
